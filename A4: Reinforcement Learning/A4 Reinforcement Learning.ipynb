{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02538519, -0.04439962, -0.01698877, -0.00198114])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    obs, rew, done, info = env.step(env.action_space.sample()) # take a random action\n",
    "    if done:\n",
    "        env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amandahutter/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for i in range(500):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n",
    "    time.sleep(0.02)\n",
    "    if i%10==0: print(i)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP Toolbox Example\n",
    "https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creates transition probability P, size (Action X State X State)\n",
    "- Creates Reward Matrix R, size (State X Action)\n",
    "- Action is either WAIT (Action = 0) or CUT (Action = 1). There is some probability p that the fire burns the forest. \n",
    "- The states of the forest are the ages of how old the forest is since last cut or burn, where S = {0, 1, ..., S-1}\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html\n",
    "\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "\n",
    "#P, R = mdptoolbox.example.forest()\n",
    "P, R = mdptoolbox.example.forest(S = 3, r1 = 4, r2 = 2, p = 0.1, is_sparse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S: The number of states, the number of years old the forest can be \n",
    "- r1: the reward when the forest is in its oldest state and action WAIT is performed \n",
    "- r2: the reward whne the forest is in its oldest state and action CUT is performed\n",
    "- p: the probability that a wild fire occurs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.9, 0. ],\n",
       "        [0.1, 0. , 0.9],\n",
       "        [0.1, 0. , 0.9]],\n",
       "\n",
       "       [[1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. ]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.9, 0. ],\n",
       "       [0.1, 0. , 0.9],\n",
       "       [0.1, 0. , 0.9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability that if we take action 0, in a given state, which state we land in\n",
    "P[0, :, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability that if we take action 0, in a given state, which state we land in\n",
    "P[1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [4., 2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 4.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If in a state, and take action 0, what is our reward\n",
    "R[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If in the state, and take action 1, what is our reward\n",
    "R[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Psp, Rsp = mdptoolbox.example.forest(is_sparse=True)\n",
    "\n",
    "len(Psp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Psp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Psp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [4., 2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sparse representation of P and R is identical to the non sparse version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Psp[0].todense() == P[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Rsp == R).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Policy Iteration on the Forest MDP\n",
    "https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9) # P = Transitions, R = Reward, 0.9 = Discount \n",
    "pi.run()\n",
    "\n",
    "expected = (26.244000000000014, 29.484000000000016, 33.484000000000016)\n",
    "all(expected[k] - pi.V[k] < 1e-12 for k in range(len(expected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.2) # P = Transitions, R = Reward, 0.9 = Discount \n",
    "pi.run()\n",
    "\n",
    "expected = (26.244000000000014, 29.484000000000016, 33.484000000000016)\n",
    "all(expected[k] - pi.V[k] < 1e-12 for k in range(len(expected)))\n",
    "\n",
    "pi.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Value Iteration on the Forest MDP https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.96)\n",
    "vi.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = (5.93215488, 9.38815488, 13.38815488)\n",
    "\n",
    "all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n",
    "\n",
    "vi.policy # Tuple shows which action maximizes the value in this state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi.iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 0)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.20) # Transition prob maatrix, reward matrix, then discount factor \n",
    "vi.verbose\n",
    "vi.run()\n",
    "print(vi.policy)\n",
    "print(vi.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add more states to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html\n",
    "\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "\n",
    "P, R = mdptoolbox.example.forest(S = 10, r1 = 4, r2 = 2, p = 0.1, is_sparse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.9, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0.9, 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0.9, 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0.9, 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0.9, 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0.9, 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9]],\n",
       "\n",
       "       [[1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [4., 2.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy Iteration with bigger state amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Iters to Converge 9\n"
     ]
    }
   ],
   "source": [
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9) # P = Transitions, R = Reward, 0.9 = Discount \n",
    "pi.run()\n",
    "\n",
    "expected = (26.244000000000014, 29.484000000000016, 33.484000000000016)\n",
    "all(expected[k] - pi.V[k] < 1e-12 for k in range(len(expected)))\n",
    "\n",
    "print(\"Optimal Policy\",pi.policy)\n",
    "print(\"Iters to Converge\",pi.iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value Iteration with bigger state amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Optimal Policy (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Iters to Converge: 16\n"
     ]
    }
   ],
   "source": [
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.90)\n",
    "print(vi.verbose)\n",
    "vi.run()\n",
    "\n",
    "expected = (5.93215488, 9.38815488, 13.38815488)\n",
    "all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n",
    "\n",
    "print(\"Optimal Policy\",vi.policy) # Tuple shows which action maximizes the value in this state\n",
    "print(\"Iters to Converge:\",vi.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q Learning on Forest Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 0, 1, 0, 1, 0, 0, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "#P, R = mdptoolbox.example.forest(S = 10, r1 = 4, r2 = 2, p = 0.1, is_sparse = False)\n",
    "\n",
    "ql = mdptoolbox.mdp.QLearning(P, R, 0.9)\n",
    "ql.run()\n",
    "#print(ql.Q)\n",
    "\n",
    "print(ql.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop Through Different State Sizes and Compare Difference in Convergence Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State size: 5\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 4  <  6\n",
      "VI is faster to converge 0.0015799999237060547  <  0.0022399425506591797\n",
      "PI has higher reward 29.208852400000016  >  15.482564617000001\n",
      "29.208852400000016\n",
      "15.482564617000001\n",
      "2.1289947463987993\n",
      "\n",
      "State size: 10\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 9  <  16\n",
      "VI is faster to converge 0.0008697509765625  <  0.0036683082580566406\n",
      "PI has higher reward 23.89652993194315  >  21.664850965110947\n",
      "23.89652993194315\n",
      "21.664850965110947\n",
      "5.0592475028361115\n",
      "\n",
      "State size: 20\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 10  <  39\n",
      "VI is faster to converge 0.004132747650146484  <  0.006227970123291016\n",
      "PI has higher reward 23.172433847048566  >  23.089675091923866\n",
      "23.172433847048566\n",
      "23.089675091923866\n",
      "8.009135384309829\n",
      "\n",
      "State size: 100\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 10  <  39\n",
      "VI is faster to converge 0.001209259033203125  <  0.008954048156738281\n",
      "PI has higher reward 23.172433847048566  >  23.089675091923866\n",
      "23.172433847048566\n",
      "23.089675091923866\n",
      "0.1377719140251788\n",
      "\n",
      "State size: 1000\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 10  <  39\n",
      "VI is faster to converge 0.014265060424804688  <  0.17009496688842773\n",
      "PI has higher reward 23.172433847048566  >  23.089675091923866\n",
      "23.172433847048566\n",
      "23.089675091923866\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#state_size = [10, 100, 1000, 10000]\n",
    "state_size = [5, 10, 20, 100, 1000]\n",
    "\n",
    "for ss in state_size:\n",
    "    \n",
    "    P, R = mdptoolbox.example.forest(S = ss, r1 = 4, r2 = 2, p = 0.1, is_sparse = False)\n",
    "    \n",
    "    # Policy Iteration \n",
    "    pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9) # P = Transitions, R = Reward, 0.9 = Discount \n",
    "    pi.run()\n",
    "    print()\n",
    "    print(\"State size:\", ss)\n",
    "    #print(\"Optimal Policy\",pi.policy)\n",
    "    #print(\"Iters to Converge\",pi.iter)\n",
    "    #print(\"PI Time:\",pi.time)\n",
    "    #print(pi.V)\n",
    "    #print(pi.policy)\n",
    "    \n",
    "    # Value Iteration \n",
    "    vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n",
    "    vi.run()\n",
    "    #print(\"Optimal Policy\",vi.policy) # Tuple shows which action maximizes the value in this state\n",
    "    #print(\"Iters to Converge:\",vi.iter)\n",
    "    #print(\"VI Time:\",vi.time)\n",
    "    #print(vi.V)\n",
    "    #print(vi.policy)\n",
    "    \n",
    "    ql = mdptoolbox.mdp.QLearning(P, R, 0.9)\n",
    "    ql.run()\n",
    "    ql.Q\n",
    "    #print(ql.V)\n",
    "    #print(ql.policy)\n",
    "    \n",
    "    print(\"Did VI and PI Give the same policy?\",vi.policy == pi.policy)\n",
    "    print(\"QLearning and PI?\",ql.policy == pi.policy)\n",
    "    print(\"QLearning and VI\",ql.policy == vi.policy)\n",
    "\n",
    "    if pi.iter < vi.iter:\n",
    "        print(\"PI requires fewer iterations to converge\", pi.iter , \" < \", vi.iter)\n",
    "    else:\n",
    "        print(\"VI requires fewer iterations to converge\", vi.iter , \" < \", pi.iter)\n",
    "    \n",
    "    if pi.time < vi.time:\n",
    "        print(\"PI is faster to converge\", pi.time , \" < \", vi.time)\n",
    "    else:\n",
    "        print(\"VI is faster to converge\", vi.time , \" < \", pi.time)\n",
    "    \n",
    "    if pi.V[-1] > vi.V[-1]:\n",
    "        print(\"PI has higher reward\", pi.V[-1] , \" > \", vi.V[-1])\n",
    "    else:\n",
    "        print(\"VI has higher reward\", vi.V[-1] , \" > \", pi.V[-1])\n",
    "    print(pi.V[-1])\n",
    "    print(vi.V[-1])\n",
    "    print(ql.V[-1])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridworld problem from Open Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S: Initial State\n",
    "- F: Frozen Lake\n",
    "- H: Hole\n",
    "- G: The Goal\n",
    "- Red Square: Current Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Create the Frozen Lake Environment \n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Put into initial state\n",
    "env.reset()\n",
    "\n",
    "# Print the State\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Observation Space: Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "print(\"Observation Space:\",env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Dummy to randomly play Frozen Lake\n",
    "\n",
    "MAX_ITERATIONS = 10\n",
    " \n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()\n",
    "env.render()\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    random_action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(\n",
    "       random_action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
