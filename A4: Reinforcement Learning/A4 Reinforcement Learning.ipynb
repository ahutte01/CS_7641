{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "Episode finished after 12 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "Episode finished after 7 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "Episode finished after 15 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "Episode finished after 4 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "Episode finished after 7 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "Episode finished after 5 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "Episode finished after 3 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "Episode finished after 4 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "Episode finished after 4 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "Episode finished after 3 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "9\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "9\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "10\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "9\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "Episode finished after 22 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "Episode finished after 6 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "Episode finished after 2 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "2\n",
      "  (Left)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "6\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "10\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "14\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "13\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "14\n",
      "Episode finished after 10 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "Episode finished after 3 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "9\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "10\n",
      "  (Up)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "6\n",
      "  (Up)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "2\n",
      "  (Up)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "3\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "3\n",
      "Episode finished after 12 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "Episode finished after 4 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "9\n",
      "  (Up)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "8\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "Episode finished after 29 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "Episode finished after 2 timesteps\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "1\n",
      "Episode finished after 6 timesteps\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        print(observation)\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP Toolbox Example\n",
    "https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creates transition probability P, size (Action X State X State)\n",
    "- Creates Reward Matrix R, size (State X Action)\n",
    "- Action is either WAIT (Action = 0) or CUT (Action = 1). There is some probability p that the fire burns the forest. \n",
    "- The states of the forest are the ages of how old the forest is since last cut or burn, where S = {0, 1, ..., S-1}\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html\n",
    "\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "\n",
    "#P, R = mdptoolbox.example.forest()\n",
    "P, R = mdptoolbox.example.forest(S = 3, r1 = 4, r2 = 2, p = 0.1, is_sparse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S: The number of states, the number of years old the forest can be \n",
    "- r1: the reward when the forest is in its oldest state and action WAIT is performed \n",
    "- r2: the reward whne the forest is in its oldest state and action CUT is performed\n",
    "- p: the probability that a wild fire occurs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.9, 0. ],\n",
       "        [0.1, 0. , 0.9],\n",
       "        [0.1, 0. , 0.9]],\n",
       "\n",
       "       [[1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. ],\n",
       "        [1. , 0. , 0. ]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.9, 0. ],\n",
       "       [0.1, 0. , 0.9],\n",
       "       [0.1, 0. , 0.9]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability that if we take action 0, in a given state, which state we land in\n",
    "P[0, :, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability that if we take action 0, in a given state, which state we land in\n",
    "P[1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [4., 2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 4.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If in a state, and take action 0, what is our reward\n",
    "R[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If in the state, and take action 1, what is our reward\n",
    "R[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Psp, Rsp = mdptoolbox.example.forest(is_sparse=True)\n",
    "\n",
    "len(Psp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Psp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x3 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Psp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [4., 2.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sparse representation of P and R is identical to the non sparse version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Psp[0].todense() == P[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Rsp == R).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Policy Iteration on the Forest MDP\n",
    "https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9) # P = Transitions, R = Reward, 0.9 = Discount \n",
    "pi.run()\n",
    "\n",
    "expected = (26.244000000000014, 29.484000000000016, 33.484000000000016)\n",
    "all(expected[k] - pi.V[k] < 1e-12 for k in range(len(expected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.2) # P = Transitions, R = Reward, 0.9 = Discount \n",
    "pi.run()\n",
    "\n",
    "expected = (26.244000000000014, 29.484000000000016, 33.484000000000016)\n",
    "all(expected[k] - pi.V[k] < 1e-12 for k in range(len(expected)))\n",
    "\n",
    "pi.policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Value Iteration on the Forest MDP https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.96)\n",
    "vi.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected = (5.93215488, 9.38815488, 13.38815488)\n",
    "\n",
    "all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n",
    "\n",
    "vi.policy # Tuple shows which action maximizes the value in this state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi.iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 0)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.20) # Transition prob maatrix, reward matrix, then discount factor \n",
    "vi.verbose\n",
    "vi.run()\n",
    "print(vi.policy)\n",
    "print(vi.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add more states to Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pymdptoolbox.readthedocs.io/en/latest/api/mdp.html\n",
    "\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "\n",
    "P, R = mdptoolbox.example.forest(S = 10, r1 = 4, r2 = 2, p = 0.1, is_sparse = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.1, 0.9, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0.9, 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0.9, 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0.9, 0. , 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0.9, 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0.9, 0. , 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. , 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9, 0. ],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9],\n",
       "        [0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.9]],\n",
       "\n",
       "       [[1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "        [1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [4., 2.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy Iteration with bigger state amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Iters to Converge 9\n"
     ]
    }
   ],
   "source": [
    "pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9) # P = Transitions, R = Reward, 0.9 = Discount \n",
    "pi.run()\n",
    "\n",
    "expected = (26.244000000000014, 29.484000000000016, 33.484000000000016)\n",
    "all(expected[k] - pi.V[k] < 1e-12 for k in range(len(expected)))\n",
    "\n",
    "print(\"Optimal Policy\",pi.policy)\n",
    "print(\"Iters to Converge\",pi.iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value Iteration with bigger state amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Optimal Policy (0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Iters to Converge: 16\n"
     ]
    }
   ],
   "source": [
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.90)\n",
    "print(vi.verbose)\n",
    "vi.run()\n",
    "\n",
    "expected = (5.93215488, 9.38815488, 13.38815488)\n",
    "all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n",
    "\n",
    "print(\"Optimal Policy\",vi.policy) # Tuple shows which action maximizes the value in this state\n",
    "print(\"Iters to Converge:\",vi.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q Learning on Forest Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 1, 0, 1, 0, 0, 0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "#P, R = mdptoolbox.example.forest(S = 10, r1 = 4, r2 = 2, p = 0.1, is_sparse = False)\n",
    "\n",
    "ql = mdptoolbox.mdp.QLearning(P, R, 0.9)\n",
    "ql.run()\n",
    "#print(ql.Q)\n",
    "\n",
    "print(ql.policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loop Through Different State Sizes and Compare Difference in Convergence Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State size: 5\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 4  <  6\n",
      "VI is faster to converge 0.0004391670227050781  <  0.0028219223022460938\n",
      "PI has higher reward 29.208852400000016  >  15.482564617000001\n",
      "29.208852400000016\n",
      "15.482564617000001\n",
      "10.080001565104842\n",
      "\n",
      "State size: 10\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 9  <  16\n",
      "VI is faster to converge 0.0008089542388916016  <  0.003010988235473633\n",
      "PI has higher reward 23.89652993194315  >  21.664850965110947\n",
      "23.89652993194315\n",
      "21.664850965110947\n",
      "3.332134772529941\n",
      "\n",
      "State size: 20\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 10  <  39\n",
      "VI is faster to converge 0.0033452510833740234  <  0.004821062088012695\n",
      "PI has higher reward 23.172433847048566  >  23.089675091923866\n",
      "23.172433847048566\n",
      "23.089675091923866\n",
      "8.684051382856067\n",
      "\n",
      "State size: 100\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 10  <  39\n",
      "VI is faster to converge 0.0016598701477050781  <  0.010624885559082031\n",
      "PI has higher reward 23.172433847048566  >  23.089675091923866\n",
      "23.172433847048566\n",
      "23.089675091923866\n",
      "0.33604157456338046\n",
      "\n",
      "State size: 1000\n",
      "Did VI and PI Give the same policy? True\n",
      "QLearning and PI? False\n",
      "QLearning and VI False\n",
      "PI requires fewer iterations to converge 10  <  39\n",
      "VI is faster to converge 0.014875173568725586  <  0.19577407836914062\n",
      "PI has higher reward 23.172433847048566  >  23.089675091923866\n",
      "23.172433847048566\n",
      "23.089675091923866\n",
      "0.2770244349439682\n"
     ]
    }
   ],
   "source": [
    "#state_size = [10, 100, 1000, 10000]\n",
    "state_size = [5, 10, 20, 100, 1000]\n",
    "\n",
    "for ss in state_size:\n",
    "    \n",
    "    P, R = mdptoolbox.example.forest(S = ss, r1 = 4, r2 = 2, p = 0.1, is_sparse = False)\n",
    "    \n",
    "    # Policy Iteration \n",
    "    pi = mdptoolbox.mdp.PolicyIteration(P, R, 0.9) # P = Transitions, R = Reward, 0.9 = Discount \n",
    "    pi.run()\n",
    "    print()\n",
    "    print(\"State size:\", ss)\n",
    "    #print(\"Optimal Policy\",pi.policy)\n",
    "    #print(\"Iters to Converge\",pi.iter)\n",
    "    #print(\"PI Time:\",pi.time)\n",
    "    #print(pi.V)\n",
    "    #print(pi.policy)\n",
    "    \n",
    "    # Value Iteration \n",
    "    vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n",
    "    vi.run()\n",
    "    #print(\"Optimal Policy\",vi.policy) # Tuple shows which action maximizes the value in this state\n",
    "    #print(\"Iters to Converge:\",vi.iter)\n",
    "    #print(\"VI Time:\",vi.time)\n",
    "    #print(vi.V)\n",
    "    #print(vi.policy)\n",
    "    \n",
    "    ql = mdptoolbox.mdp.QLearning(P, R, 0.9)\n",
    "    ql.run()\n",
    "    ql.Q\n",
    "    #print(ql.V)\n",
    "    #print(ql.policy)\n",
    "    \n",
    "    print(\"Did VI and PI Give the same policy?\",vi.policy == pi.policy)\n",
    "    print(\"QLearning and PI?\",ql.policy == pi.policy)\n",
    "    print(\"QLearning and VI\",ql.policy == vi.policy)\n",
    "\n",
    "    if pi.iter < vi.iter:\n",
    "        print(\"PI requires fewer iterations to converge\", pi.iter , \" < \", vi.iter)\n",
    "    else:\n",
    "        print(\"VI requires fewer iterations to converge\", vi.iter , \" < \", pi.iter)\n",
    "    \n",
    "    if pi.time < vi.time:\n",
    "        print(\"PI is faster to converge\", pi.time , \" < \", vi.time)\n",
    "    else:\n",
    "        print(\"VI is faster to converge\", vi.time , \" < \", pi.time)\n",
    "    \n",
    "    if pi.V[-1] > vi.V[-1]:\n",
    "        print(\"PI has higher reward\", pi.V[-1] , \" > \", vi.V[-1])\n",
    "    else:\n",
    "        print(\"VI has higher reward\", vi.V[-1] , \" > \", pi.V[-1])\n",
    "    print(pi.V[-1])\n",
    "    print(vi.V[-1])\n",
    "    print(ql.V[-1])\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridworld problem from Open Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S: Initial State\n",
    "- F: Frozen Lake\n",
    "- H: Hole\n",
    "- G: The Goal\n",
    "- Red Square: Current Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Create the Frozen Lake Environment \n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Put into initial state\n",
    "env.reset()\n",
    "\n",
    "# Print the State\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Actions: 4\n",
      "Observation Space: Discrete(16)\n",
      "Size: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space:\", env.action_space)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Actions:\",n_actions)\n",
    "\n",
    "print(\"Observation Space:\",env.observation_space)\n",
    "n_states = env.observation_space.n\n",
    "print(\"Size:\",n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "0\n",
      "0.0\n",
      "False\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "4\n",
      "0.0\n",
      "False\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "8\n",
      "0.0\n",
      "False\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "9\n",
      "0.0\n",
      "False\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "5\n",
      "0.0\n",
      "True\n",
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Dummy to randomly play Frozen Lake\n",
    "\n",
    "MAX_ITERATIONS = 10\n",
    " \n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "env.reset()\n",
    "env.render()\n",
    "for i in range(MAX_ITERATIONS):\n",
    "    random_action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(\n",
    "       random_action)\n",
    "    print(new_state)\n",
    "    print(reward)\n",
    "    print(done)\n",
    "    #print(info)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://learning.oreilly.com/library/view/reinforcement-learning-algorithms/9781789131116/ab06aa68-01f9-481e-94ac-4c6748c3b858.xhtml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: 4\n",
      "Size: 16\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "#env.reset()\n",
    "#env.render()\n",
    "env = env.unwrapped\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "print(\"Actions:\",n_actions)\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "print(\"Size:\",n_states)\n",
    "\n",
    "V = np.zeros(n_states)\n",
    "policy = np.zeros(n_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in Policy Iteration and Value Iteration\n",
    "def eval_state_action(V, s, a, gamma = 0.99):\n",
    "    return np.sum([p* (rew + gamma*V[next_s]) for p, next_s, rew, _ in env.P[s][a]]) # env.P is a dictionary with info about enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(V, policy, epsilon = 0.0001):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(n_states):\n",
    "            old_v = V[s]\n",
    "            V[s] = eval_state_action(V, s, policy[s])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "        if delta < epsilon:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(V, policy):\n",
    "    policy_stable = True\n",
    "    for s in range(n_states):\n",
    "        old_a = policy[s]\n",
    "        policy[s] = np.argmax( [eval_state_action(V, s, a) for a in range(n_actions)] )\n",
    "        if old_a != policy[s]:\n",
    "            policy_stable = False\n",
    "    return policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes_PI(env, V, policy, num_games = 1000):\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        while not done:\n",
    "            next_state, reward, done, _ = env.step(policy[state])\n",
    "            state = next_state\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "    print(\"Won \", tot_rew, \" of \", num_games, \" games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the main cycle to do 1 step of policy evaluation and 1 step of policy improvement\n",
    "policy_stable = False\n",
    "iteration = 0\n",
    "while not policy_stable:\n",
    "    policy_evaluation(V, policy)\n",
    "    policy_stable = policy_improvement(V, policy)\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after iteration: 1\n",
      "Won  836.0  of  1000  games\n",
      "[[0.54118851 0.49768315 0.46938028 0.45543752]\n",
      " [0.55770637 0.         0.35778072 0.        ]\n",
      " [0.59119068 0.64264127 0.6148228  0.        ]\n",
      " [0.         0.74141828 0.8626849  0.        ]]\n",
      "[[0. 3. 3. 3.]\n",
      " [0. 0. 0. 0.]\n",
      " [3. 1. 0. 0.]\n",
      " [0. 2. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Converged after iteration:\", iteration)\n",
    "run_episodes_PI(env, V, policy)\n",
    "print(V.reshape((4,4))) # Make game board \n",
    "print(policy.reshape((4,4))) # Game board size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(epsilon = 0.0001):\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        # Update value for each state \n",
    "        for s in range(n_states):\n",
    "            old_v = V[s]\n",
    "            V[s] = np.max([eval_state_action(V, s, a) for a in range(n_actions)])\n",
    "            delta = max(delta, np.abs(old_v - V[s]))\n",
    "        # When stable, break\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            print(\"Iteration:\", iteration, \"Delta: \", np.round(delta,5))\n",
    "        iteration += 1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes_VI(env, V, num_games = 100):\n",
    "    tot_rew = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    for _ in range(num_games):\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax([eval_state_action(V, state, a) for a in range(n_actions)])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            tot_rew += reward \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "    print(\"Won \", tot_rew, \" of \", num_games, \" games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Won  808.0  of  1000  games\n",
      "[[0.54133406 0.49787996 0.46961266 0.45568792]\n",
      " [0.55783594 0.         0.35788083 0.        ]\n",
      " [0.5912967  0.64271799 0.61489039 0.        ]\n",
      " [0.         0.74147114 0.86271159 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V = value_iteration(epsilon = 0.0001)\n",
    "\n",
    "run_episodes_VI(env, V, 1000)\n",
    "\n",
    "print(V.reshape((4,4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
